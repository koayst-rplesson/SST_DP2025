{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/Day_02/L12/L12.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab (preferred).\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `SST_DP2025`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f4c2-8506-415c-a295-041ee40d9eda",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at Python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.tx')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab.\n",
    "\n",
    "## Tip\n",
    "The output from LangChain/LangGraph is a complex data structure and can be difficult to read/understand. Use [Python Formatter](https://codebeautify.org/python-formatter-beautifier) to format the output for easy reading."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c79ed1d-cc1e-4712-93ce-695496907dd7",
   "metadata": {},
   "source": [
    "# Lesson 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a344c6-9289-4351-9677-8e7f778c7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain\n",
    "%pip install --quiet -U langgraph\n",
    "%pip install --quiet -U langchain-openai\n",
    "%pip install --quiet -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f2767-9f87-48e8-b1b7-c0b952d2cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain              0.3.11\n",
    "# langgraph              0.2.59\n",
    "# langchain-core         0.3.24\n",
    "# langchain-openai       0.2.12\n",
    "# langchain-community    0.3.12\n",
    "# openai                 1.57.2\n",
    "# pydantic               2.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7281bdb-a238-4cad-abac-6679aa169ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# setup the OpenAI API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc02a79-8283-4235-b570-c813d51bff91",
   "metadata": {},
   "source": [
    "## LangGraph\n",
    "\n",
    "LangGraph is an abstraction or framework designed to model workflows, processes, or computations as directed graphs. It is often used in systems that require structured and modular workflows, particularly in natural language processing (NLP), AI pipelines, or decision-making applications.\n",
    "\n",
    "In a LangGraph-style architecture, nodes represent individual tasks or operations, and edges define the flow of information between these tasks.  Every node is a function. Each node in the graph encapsulates a specific piece of functionality. It takes an input, performs a computation or action and produces an output.\n",
    "\n",
    "Edge as data flow. The connections between nodes represent the flow of data or the results from one function to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c858a1-d480-4869-be01-a06c2f077b78",
   "metadata": {},
   "source": [
    "## Basic Chatbot\n",
    "\n",
    "Let's build a basic chatbot.\n",
    "\n",
    "References: [LangGraph Quick Start ](https://langchain-ai.github.io/langgraph/tutorials/introduction/#setup), [LangChain Academy](https://academy.langchain.com/)</br></br>\n",
    "This chatbot will respond directly to user message. Though simple, it will illustrate the core concepts of building with LangGraph.\n",
    "\n",
    "Start by creating a `StateGraph` that defines the \"state\" machine of the chatbot.  `Node` representing the LLM is added and `edges` to specify how the bot should transition between these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af928c-135a-41b3-a6d6-35c0853afb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8b32b-c064-4b19-88db-579b10ddb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: State inherits from TypedDict\n",
    "class State(_____):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    #\n",
    "    # pre-built 'add_message' is also known as 'reducer'\n",
    "    # reducer allows us to specify how stat updates are performed\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643bc42-a928-4fbb-94e9-47005d0f876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the graph\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab754e-4c7c-4e43-bd8a-32ff54d89d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the LLM model\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40d87a-a35f-4714-b5a4-e0cded9db7c3",
   "metadata": {},
   "source": [
    "### Create the 'chatbot' node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53c9c1-4b9f-475b-84b8-f6dc8d2248df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice how the chatbot node function takes the current State as input and returns a dictionary containing an updated \n",
    "# messages list under the key \"messages\". This is the basic pattern for all LangGraph node functions.\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\n",
    "        'messages' : [model.invoke(state['messages'])]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505f22c-ea22-412c-8f83-1dc31a9ee0e9",
   "metadata": {},
   "source": [
    "### Add 'chatbot' node to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a45ef-ebe9-4d33-909b-ba68b1e25e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first argument is the unique node name\n",
    "# the second argument is the function or object that will be called whenever the node is used\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f8962-d782-4541-8be9-88e4f5da033c",
   "metadata": {},
   "source": [
    "### Connect 'START' node to 'chatbot' node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2647f-c215-48e2-8a48-179a15622d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(START, 'chatbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd81643-8431-4df0-b9f7-f45eaa2cc417",
   "metadata": {},
   "source": [
    "### Connect 'chatbot' node to 'END' node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04404b-79d5-4da7-893f-b595167984da",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c86cc-9308-49e0-9c0d-60475e56c222",
   "metadata": {},
   "source": [
    "### Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd88ef6-c4ae-4f13-a3ee-1eaa42b66e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compile the graph\n",
    "\n",
    "graph = graph_builder._____()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482848a-ea36-4687-844b-2bb70e2b7beb",
   "metadata": {},
   "source": [
    "### Display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae66e7-2243-4ab4-b8d5-580210f099f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# visualise the graph as a Mermaid diagram\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n",
    "\n",
    "# notice in the diagram, the arrow is a solid line\n",
    "# dotted line denotes conditional edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8597eba-d55d-4402-a7c9-23e5fbc9c7b2",
   "metadata": {},
   "source": [
    "### Run the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46e396-0ad7-4682-a387-5255a84c3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390fd3c7-5806-4091-9d9a-f08c9ce6fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break\n",
    "\n",
    "# User: What is quantum computing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f758d-133e-4143-8f79-63050dfa4af8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c6e47-994e-4c3b-8cfb-86b442b4a6c0",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Tools are useful whenever you want a model to interact with external systems.</br>\n",
    "\n",
    "In order for the chatbot to handle queries not \"from memory\", we need to integrate a web search tool. The bot will use Tavily Search to find the relevant information and provide better responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968aac2f-d740-494c-a118-bc75855918f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542df74-183d-4ac0-9bff-913bb1751d48",
   "metadata": {},
   "source": [
    "The code pattern for MessagesState when we want to appends messages to the list, rather than overwriting them is:\n",
    "\n",
    "```\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "```\n",
    "\n",
    "Since having a list of messages in graph state is so common, `LangGraph` has a pre-built `MessageState` as shown in the class definition `MessagesState`. ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c7925-5796-4c32-a9e4-c56f94371c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagesState(MessagesState):\n",
    "    # add any keys needed beyond messages, which is pre-built\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9daeaa8-95ca-4369-9577-460c8a54a943",
   "metadata": {},
   "source": [
    "### A Simple Example of Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec4f33-82ae-45cd-a261-0d579729822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages.human import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f169d0-26b8-4857-9130-9fb19a43ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b\n",
    "\n",
    "    Args:\n",
    "    a: first int\n",
    "    b: second int\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9939c56-762c-4167-a99c-360e1ed67800",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be332ffc-3569-4563-a8d5-014ba982f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice you call bind_tools using a list\n",
    "# that's mean you can add in more than one tool into the list\n",
    "\n",
    "model_with_tools = model.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552e84f-b7bc-4a96-b0a8-ee807b2263ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test the tool\n",
    "messages = model_with_tools.invoke(\n",
    "    [HumanMessage(content=f\"What is 4 multiplied by 8?\")]\n",
    ")\n",
    "\n",
    "tool = {'multiply' : multiply}[messages.tool_calls[0]['name'].lower()]\n",
    "print(tool(**(messages.tool_calls[0]['args'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfda45c-d020-4f83-b7ec-060ec5b9d55e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec528b8-2975-46a9-8401-5a4f5b406d56",
   "metadata": {},
   "source": [
    "### Use `MessagesState` with a Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e63243-7c76-448a-92b7-498b95063fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299bc4ac-fe30-4b03-b4de-680056c6d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b\n",
    "\n",
    "    Args:\n",
    "    a: first int\n",
    "    b: second int\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f2aa9-4b2b-4b4b-95e6-f0d2941bcaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "model_with_tools = model.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b97381-97ef-4831-af6b-8fe6a8fc5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_calling_model(state: MessagesState):\n",
    "    response = {\"messages\" : [model_with_tools.invoke(state['messages'])]}\n",
    "    print(f\">>> tool_calling_model: {response}\\n\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03495cb-992f-4628-be9e-3aac22743cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tools_condition)\n",
    "\n",
    "# accordingly, tools_condition is a perbuilt tool that \"Use in the conditional_edge to route to the ToolNode \n",
    "# if the last message has tool calls. Otherwise, route to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65b03b-509a-4564-ac9c-148e199f5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Graph\n",
    "\n",
    "# TODO: build the graph by filling in the blanks\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node('_____', tool_calling_model)\n",
    "graph_builder.add_node('_____', ToolNode([multiply]))\n",
    "\n",
    "graph_builder.add_edge(_____, '_____')\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"tool_calling_model\",\n",
    "    tools_condition\n",
    ")\n",
    "                       \n",
    "graph_builder.add_edge('_____', _____)\n",
    "                       \n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 1. START is connected to 'tool_calling_model'\n",
    "# 2. 'tool_calling_model' is connected to 'tool_condition'\n",
    "# 3. When 'tool_calling_model' is passed a user request, it will return a response indicating tool calls is requested or not\n",
    "# 4. 'tool_condition' will route to 'tools' (ToolNode) to perform \"multiply\" if there is tool call else to END\n",
    "# 5. 'tools' is connected to END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d56b8-6e5b-4283-812c-80eccbda9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224cec8-be70-40a4-b9aa-f3a236db7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(graph_builder.add_conditional_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf79ff-4333-4631-b49a-f808aaaa3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the graph with message 'Hello World'\n",
    "\n",
    "messages = graph.invoke({\"messages\" : HumanMessage(content = \"Hello World\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c3b0f-0f2f-41d3-96fe-d5b29a2d8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the graph with message 'Multiply 4 and 8'\n",
    "\n",
    "messages = graph.invoke({\"messages\" : HumanMessage(content = \"What is 4 multiplied by 8?\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()\n",
    "\n",
    "# notice in 'additional_kwargs`, there is a 'tool_calls' ↓"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75239a75-4e42-4c4f-a031-86fa63b191ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f6564-9dc3-4714-93ae-0dc39d28cf9c",
   "metadata": {},
   "source": [
    "## Agent\n",
    "An agent is an autonomous entity that can execute tasks, interact with various components, and make decisions based on the given instructions or data.\n",
    "\n",
    "- Agents can perform tasks independently, such as querying data, executing code, or interacting with APIs\n",
    "- An agent typically works to achieve a specific objective, such as answering a question, processing input, or generating content\n",
    "- Based on the data they receive and the rules they're programmed with, agents can decide on the next steps in a workflow\n",
    "\n",
    "You have just encountered a generic 'agent' using a 'router'. Next we are going to build a 'ReAct' agent.  ReAct (short for Reasoning + Acting) is a paradigm designed to enable Large Language Models (LLMs) to perform tasks that require multi-step reasoning and interaction with external tools or environments\n",
    "\n",
    "- act - let the model call specific tools\n",
    "- feedback - pass the tool output back to the model\n",
    "- reason - the model reasons from the tool output and decide what to do next "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c7051-fb0d-45bd-bccb-dd786857f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e46c58-7b24-4401-8870-f0e35bfa42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the nodes function\n",
    "\n",
    "def multiply(a : int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add (a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"\n",
    "    Divide a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b642e2-e62d-49f4-aae8-3bed5aea4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini'\n",
    ")\n",
    "\n",
    "# TODO: add the missing tool in the list\n",
    "arithmetic_tools = [add, multiply, _____]\n",
    "model_with_tools = model.bind_tools(arithmetic_tools)\n",
    "\n",
    "# system message\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a helful assistant tasked with performing arithmetic operations.\"\n",
    ")\n",
    "\n",
    "# helpful assistant node\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\" : [model_with_tools.invoke([sys_msg] + state['messages'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe55c05-3b86-42d4-9c10-e4f25fbd02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "\n",
    "# TODO: build the graph by filling in the blanks\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node('_____', assistant)\n",
    "graph_builder.add_node('_____', ToolNode(arithmetic_tools))\n",
    "\n",
    "graph_builder.add_edge(START, '_____')\n",
    "graph_builder.add_conditional_edges(\n",
    "    'assistant',\n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "graph_builder.add_edge('_____', '_____')\n",
    "react_graph = graph_builder.compile()\n",
    "\n",
    "# show graph\n",
    "# notice there is not explicit connection from 'assistant' to end\n",
    "# remember from documentation of 'add_conditional_edges':\n",
    "# Use in the conditional_edge to route to the ToolNode if the last message has tool calls. Otherwise, route to the end.\n",
    "\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0bb25-c9b3-4d79-be19-52e469877f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_msg = [HumanMessage(content='Add 3 and 4. Multiply the output by 2. Divide the output by 5')]\n",
    "test_msg = react_graph.invoke({'messages' : test_msg})\n",
    "\n",
    "for m in test_msg['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b52cc-7cd0-455b-a700-65d6a0639852",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c09d8-e2c4-4efe-94ba-1ce29b59b8fe",
   "metadata": {},
   "source": [
    "## Breakpoint And Human-In-Loop (HIL)\n",
    "Breakpoints provide a simple way to stop the graph at a specific step to seek human approval before proceeding. Breakpoints are a common pattern in Human-In-Loop interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d87162-9c3e-4f84-ab1b-7c7b6b895977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d33a50-fffc-4137-9ddf-151e540f6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the nodes function\n",
    "\n",
    "def multiply(a : int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add (a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"\n",
    "    Divide a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c6d03-2330-41b6-b678-6dbca7b3140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini'\n",
    ")\n",
    "\n",
    "# TODO: add the missing tool in the list\n",
    "arithmetic_tools = [add, _____, divide]\n",
    "model_with_tools = model.bind_tools(arithmetic_tools)\n",
    "\n",
    "# system message\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a helful assistant tasked with performing arithmetic operations.\"\n",
    ")\n",
    "\n",
    "# helpful assistant node\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\" : [model_with_tools.invoke([sys_msg] + state['messages'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f53128-7412-4dfc-8db4-de07cc7eb0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "\n",
    "# TODO: build the graph by filling in the blanks\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node('assistant', _____)\n",
    "graph_builder.add_node('tools', ToolNode(a_____))\n",
    "\n",
    "graph_builder.add_edge(_____, '_____')\n",
    "graph_builder.add_conditional_edges(\n",
    "    'assistant',\n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "graph_builder.add_edge('_____', '_____')\n",
    "\n",
    "# set up memory\n",
    "memory = MemorySaver()\n",
    "\n",
    "# execution will be interrupted befoe the 'tools' node\n",
    "breakpoint_graph = graph_builder.compile(interrupt_before=['tools'], checkpointer=memory)\n",
    "\n",
    "# show graph\n",
    "# notice there is not explicit connection from 'assistant' to end\n",
    "# remember from documentation of 'add_conditional_edges':\n",
    "# Use in the conditional_edge to route to the ToolNode if the last message has tool calls. Otherwise, route to the end.\n",
    "\n",
    "display(Image(breakpoint_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d19298-2633-46e0-927c-82a83162274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\" : str(random.randint(1, 100))}}\n",
    "\n",
    "# Input\n",
    "initial_input = {\"messages\": \"Add 3 and 4. Multiply the output by 2. Divide the output by 5.\"}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in breakpoint_graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()\n",
    "\n",
    "user_approval = input(\"Do you want to continue (yes/no):\")\n",
    "\n",
    "while (user_approval.lower() == 'yes'):\n",
    "    for event in breakpoint_graph.stream(None, thread, stream_mode=\"values\"):        \n",
    "        event['messages'][-1].pretty_print()\n",
    "\n",
    "    # if there is no more node to run, stop the execution\n",
    "    # you can comment out thee code to observe the execution flow\n",
    "    # it will keep running the last operation\n",
    "    if len(breakpoint_graph.get_state(thread).next) == 0:\n",
    "        break\n",
    "\n",
    "    user_approval = input(\"Do you want to continue (yes/no):\")\n",
    "    if user_approval.lower() != 'yes':\n",
    "        print(\"Operation cancelled by user.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04ff6b-16f5-49ce-922f-c433e1b919f2",
   "metadata": {},
   "source": [
    "## Editing Graph State\n",
    "Earlier, we interrupt the graph and allow user to accept an action or not. Now we are going to modify the state. Breakpoints are also opportunities to modify the graph state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594463e-9a71-4ec7-83c1-a935529dc4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a63c4d-f5d3-4dd4-9a4a-ed7a9fe62b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the nodes function\n",
    "\n",
    "def multiply(a : int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add (a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"\n",
    "    Divide a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523aa47-839e-4668-bf29-489659cdaf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini'\n",
    ")\n",
    "\n",
    "# TODO: add the missing tools in the list\n",
    "arithmetic_tools = [_____, _____, _____]\n",
    "\n",
    "# TODO: binds the tools\n",
    "model_with_tools = model.bind_tools(_____)\n",
    "\n",
    "# system message\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a helful assistant tasked with performing arithmetic operations.\"\n",
    ")\n",
    "\n",
    "# helpful assistant node\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\" : [model_with_tools.invoke([sys_msg] + state['messages'])]}\n",
    "\n",
    "# added this for human_feedback creation\n",
    "def human_feedback(stateLMessageState):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232865e-e46d-4513-8681-1bd7b3ec517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "# TODO: build the graph by filling in the blanks\n",
    "graph_builder.add_node('assistant', _____)\n",
    "graph_builder.add_node('tools', ToolNode(_____))\n",
    "\n",
    "# added this node \"human_feedback\"\n",
    "graph_builder.add_node(\"human_feedback\", _____)\n",
    "\n",
    "graph_builder.add_edge(_____, 'human_feedback')\n",
    "graph_builder.add_edge(\"_____\", \"_____\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    'assistant',\n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "graph_builder.add_edge('_____', '_____')\n",
    "\n",
    "# set up memory\n",
    "memory = _____()\n",
    "\n",
    "# execution will be interrupted befoe the 'human_feedback' node\n",
    "breakpoint_graph = graph_builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "\n",
    "# show graph\n",
    "# notice there is not explicit connection from 'assistant' to end\n",
    "# remember from documentation of 'add_conditional_edges':\n",
    "# Use in the conditional_edge to route to the ToolNode if the last message has tool calls. Otherwise, route to the end.\n",
    "\n",
    "display(Image(breakpoint_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb1cf3-3f13-488a-a152-a79310fbbcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\" : str(random.randint(1, 100))}}\n",
    "\n",
    "# Input\n",
    "# deliberately put in a divide by 0\n",
    "initial_input = {\"messages\": \"Add 3 and 4. Multiply the output by 2. Divide the output by 0.\"}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in breakpoint_graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()\n",
    "\n",
    "user_want_to_edit = input(\"Do you want to edit the prompt (yes/no):\")\n",
    "if user_want_to_edit.lower() == 'yes':\n",
    "    new_prompt = input('Tell me the new prompt:')\n",
    "    breakpoint_graph.update_state(thread, {\"messages\" : new_prompt}, as_node=\"human_feedback\")\n",
    "\n",
    "    for event in breakpoint_graph.stream(None, thread, stream_mode=\"values\"):\n",
    "        event['messages'][-1].pretty_print()\n",
    "else:\n",
    "    print(\"Operation cancelled by user.\")\n",
    "\n",
    "# Note:\n",
    "# When ask to edit the prompt, you need to reply as it is not a replacement operation\n",
    "\n",
    "# Possibilities:\n",
    "# 1. Enter (copy text within quotes) → \"Ignore the last instruction. Do this: Add 3 and 4. Multiply the output by 2. Divide the output by 10.\"\n",
    "# 2. Enter (copy text within quotes) → \"There is a typo error. Instead of divide by 0, change it to divide by 10.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1fe7b-630c-4a37-a4ba-98633b83337f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
