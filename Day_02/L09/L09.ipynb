{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/Day_02/L09/L09.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `SST_DP2025`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f4c2-8506-415c-a295-041ee40d9eda",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.tx')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c79ed1d-cc1e-4712-93ce-695496907dd7",
   "metadata": {},
   "source": [
    "# Lesson 09\n",
    "\n",
    "- LangChain is a framework built around LLMs.\n",
    "- Framework offered as a Python or Javascript (Typescript) package.\n",
    "- Use it to build chatbots, Generative Question-Answer (GQA), summarization and much more.\n",
    "- Core idea is to “chain” together different components to create more advanced use cases around LLMs.\n",
    "- Provides developers with a comprehensive set of tools to seamlessly combine multiple prompts working with LLMs effortlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a344c6-9289-4351-9677-8e7f778c7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain\n",
    "%pip install --quiet -U langgraph\n",
    "%pip install --quiet -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f2767-9f87-48e8-b1b7-c0b952d2cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain        0.3.11\n",
    "# langgraph        0.2.59\n",
    "# langchain-core   0.3.24\n",
    "# langchain-openai 0.2.12\n",
    "# openai           1.57.2\n",
    "# pydantic         2.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7281bdb-a238-4cad-abac-6679aa169ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# setup the OpenAI API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc02a79-8283-4235-b570-c813d51bff91",
   "metadata": {},
   "source": [
    "## A Simple LLM Application\n",
    "We will be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models). It takes a sequence of message as inputs and returns chat messages as outputs.\n",
    "\n",
    "LangChain does not host any of the chat models. It depends on [third party](https://python.langchain.com/docs/integrations/chat/) LLM model providers. We will be using ChatOpenAI due to its popularity and performant. There are a few standard parameters that we can set with the chat models. Two most common are:\n",
    "- `model`: the name of the model\n",
    "- `temperature`: the sampling temperature\n",
    "\n",
    "`temperature` controls the randomness or creativity of the model's output. Low temperature (close to 0) is more deterministic and focused outputs. High temperature (close to 1) is good for creative tasks or generating varied responses.\n",
    "\n",
    "Chat models in LangChain have a number of (default methods)[https://python.langchain.com/v0.2/docs/concepts/#runnable-interface]. Most of the time, we will be using:\n",
    "- `stream`: stream back chunks of response\n",
    "- `invoke`: call the chain on the input\n",
    "  \n",
    "Chat models take messages as input. Messages have a role that describes who is saying the message and a content property. We get an `AIMessage` response upon invoking the model with messages.\n",
    "\n",
    "We can also invoke a chat model with a string. The string is converted to `HumanMessage` and then passed to the model for processing. This interface is consistent across all chat models and models are typically initialised once at the start of each notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ce11d-96fc-4aad-b307-0ec50122e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fe6ff-5fba-46d9-a05d-ec835ce93b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chat LLM model\n",
    "chat_model = ChatOpenAI(\n",
    "    # don't need this if the OpenAI API Key is stored in the environment variable\n",
    "    #openai_api_key=\"sk-proj-xxxxxxxxx\",\n",
    "\n",
    "    # TODO: we will be using gpt-4o-mini\n",
    "    model = '_____'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524decd-83e2-47ac-a9c0-8ce81fe1946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup message prompt\n",
    "text = \"What date is Singapore National Day?\"\n",
    "messages = [HumanMessage(content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ebcf8-6c39-432a-b0a5-13eefbfd9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that Chat Model takes in message objects as input and generate message object as output\n",
    "\n",
    "# TODO: invoke the LLM model\n",
    "response = chat_model._____(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324918b-ae6e-49ae-8eb3-edbd8569817d",
   "metadata": {},
   "source": [
    "## A Simple LLM Application With Prompt Template\n",
    "\n",
    "**Purpose of Prompt Template**\n",
    "- Parameterized templates that dynamically generate specific prompts based on user input or other variables.\n",
    "- By using variables in the template, you can adapt the prompt to specific scenarios without rewriting the entire prompt.\n",
    "- Keeps your application logic (like API calls or response handling) separate from the prompt text, improving code readability and maintainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53266ed8-b053-4872-8622-a5949dda4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba721d4-9855-4c15-a188-ff90b396068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise ChatModel with API key\n",
    "chat_model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini', \n",
    "\n",
    "    # TODO: set the temperature to zero point three\n",
    "    temperature = _____\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c572494-7a0a-4df2-a7e1-e7b9a55f1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template takes in raw user input ({input_language} and {output_language}) and \n",
    "# return a prompt that is ready to pass into a language model\n",
    "\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "\n",
    "# TODO: human_template defines the structure for the human message\n",
    "# \"text\" is the placeholder for user-provided input\n",
    "human_template = \"{_____}\"\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005be55-3b7f-45d6-bc9e-a7538c7d0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trsnslate English to French\n",
    "\n",
    "# TODO: fill in the appropriate language strings for input and out languages\n",
    "messages = chat_prompt.format_messages(\n",
    "    input_language = \"_____\", \n",
    "    output_language = \"_____\", \n",
    "    text = \"I love programming.\"\n",
    ")\n",
    "\n",
    "response = chat_model.invoke(messages).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80a83a-d814-4c23-9aa8-9efa33ee7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate English to Chinese\n",
    "\n",
    "# TODO: fill in the appropriate language string for input and out languages\n",
    "messages = chat_prompt.format_messages(\n",
    "    input_language = \"_____\", \n",
    "    output_language = \"_____\", \n",
    "    text = \"I love programming.\"\n",
    ")\n",
    "\n",
    "# TODO: invoke the model\n",
    "response = chat_model._____(messages).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7f5ef-ebaa-4fae-a9d0-d79254b910f6",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "`Runnable` interface is the foundation for working with LangChain components. A unit of work that can be invoked, batched, streamed, transformed and composed.\n",
    "\n",
    "**Key Methods - (Synchronouse/Asynchronous):**\n",
    "- `invoke`/`ainvoke`: Transforms a single input into an output.\n",
    "- `batch`/`abatch`: Transforms multiple inputs into outputs.\n",
    "- `stream`/`astream`: Outputs are streamed as they are produced.\n",
    "\n",
    "Streaming is typically used when interfacting with language models to enable real-time or chunked responses. The purpose of this parameter is to allow the model to stream its output piece by piece (e.g., word by word or token by token) instead of waiting for the entire response to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2f1fa-35e1-48bc-925f-0a35c686d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = chat_prompt.format_messages(\n",
    "    input_language = \"English\", \n",
    "    output_language = \"English\", \n",
    "    text = \"What is teh-c siew dai?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69e880-6403-460c-a996-e388192a19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation:\n",
    "# the output is streamed/printed piece by piece (eg. word by word or token by token)\n",
    "\n",
    "# TODO: we would like to stream the output\n",
    "for token in chat_model._____(messages):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7520039-1b31-46ca-96da-f5af86facff2",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "- From the sample codes you just run, you have learned how to create your first simple LLM application. \n",
    "- You learned how to work with language model(s) and how to create a prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6301b-ede6-4653-879e-8c47cc0b0d2b",
   "metadata": {},
   "source": [
    "## PromptTemplate And LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694c058-eb8e-4cfa-bf5c-28ebc0e75280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f62e3-cd4c-44a5-8f90-313755229e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4 does not have 'instruct' model\n",
    "# The chat models like gpt-4 and gpt-4-turbo are already instruction-following models.\n",
    "# They are designed to interpret and respond effectively to instructions provided in the conversatio\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.7,\n",
    ")\n",
    "\n",
    "# TODO: parse the output of a language model and convert it to a string\n",
    "output_parser = _____()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692ef24-ee42-4564-87ba-0002b5b191c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup template \n",
    "\n",
    "human_template = \"Write {lines} sentences about {topic}.\"\n",
    "prompt = ChatPromptTemplate.from_template(human_template)\n",
    "\n",
    "lines_topic_dict = {\n",
    "    \"lines\" : \"3\", \n",
    "    \"topic\": \"Sir Stamford Raffles\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8889c-e7f3-4096-a4df-14880c1a0bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without piping to StrOutputParser\n",
    "# StrOutputParser = OutputParser that parses LLMResult into the top likely string\n",
    "\n",
    "# TODO: prompt -> llm\n",
    "lcel_chain_01 = _____ | _____\n",
    "\n",
    "lcel_chain_01.invoke(lines_topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3fca9-f12f-4eb8-9e29-1f4fd13f91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe to StrOutputParesr\n",
    "\n",
    "# TODO: prompt -> llm -> string output parser\n",
    "lcel_chain_02 = _____ | _____ | _____\n",
    "\n",
    "lcel_chain_02.invoke(lines_topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32fd62-6e98-4294-8b46-42d8d452eee6",
   "metadata": {},
   "source": [
    "## Template\n",
    "\n",
    "### Prompt Template\n",
    "Prompt templates take as input a dictionary where each key represents a variable for the prompt template to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbbd33f-8d1e-40c9-8ecf-e39c8c36a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "# TODO: create a prompt template with \"sport\" as the placeholder\n",
    "prompt = _____(\n",
    "    input_variables = [\"sport\"],\n",
    "    template = \"I love playing {_____}\"\n",
    ")\n",
    "\n",
    "prompt.invoke({\"sport\":\"table-tennis\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ecd88-7b23-460d-b31c-02f3bb64e4a4",
   "metadata": {},
   "source": [
    "Usually you will initialise Prompts using `from_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33087d42-d382-46ef-8092-7b97e2883de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a prompt template from template\n",
    "# the two placeholders are \"country\" and \"adjective\"\n",
    "prompt = PromptTemplate._____(\n",
    "    \"I love visitng {_____} because of its {_____}\"\n",
    ")\n",
    "\n",
    "prompt.invoke({\n",
    "    \"country\" : \"Japan\",\n",
    "    \"adjective\" : \"scenery\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad56989-900d-4625-bda6-17483a6c3804",
   "metadata": {},
   "source": [
    "### MessagesPlaceholder\n",
    "Prompt template is responsible for adding a list of messages in a particular place. If the user want to pass in a list of messages that could be slotted into a particular spot, we can use `MessagePlaceholder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c6452-6998-4cf6-824d-c6adac2a7bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "\n",
    "    # place Human Message after System Message\n",
    "    MessagesPlaceholder(\"messages\")\n",
    "])\n",
    "\n",
    "prompt.invoke({\"messages\" : [HumanMessage(content=\"Hi! My name is John\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907223f-3e31-4f2e-8037-cd4a54330226",
   "metadata": {},
   "source": [
    "## LangChain Parsers / Structured Output\n",
    "\n",
    "LangChain API Reference: [LangChain output_parser](https://python.langchain.com/api_reference/langchain/output_parsers.html)\n",
    "\n",
    "### CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b3688-c28e-4cb1-a8ef-e7fd8896c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.list import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ef677-c3fc-433e-a364-787880fd2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialise a comma separated list output parser\n",
    "output_parser = _____()\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696597f-5a79-489b-b266-8f2bfebc9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"List down 5 countries that start with letter'{alphabet}'\\n{format_instructions}\",\n",
    "    input_variables=[\"alphabet\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdebfe0-b34b-46f4-9845-2c3061042161",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff65cf3-3ba9-4290-a67a-e53b1def1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(alphabet=\"S\")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3255e8-ff14-44c2-a05a-f88baa86b9ce",
   "metadata": {},
   "source": [
    "## JSON\n",
    "JSON is a lightweight, human-readable format for representing structured data. LLMs often use JSON to exchange information in a structured and consistent way. JSON is commonly used to represent and transmit structured data, making it easier to process and use in programming tasks.\n",
    "\n",
    "JSON can represent key-value pairs, lists, and nested objects, which are useful for structured outputs. Many APIs, including those of LLMs, send and receive data in JSON format. LLMs might generate JSON outputs to integrate with other systems, such as databases, web applications, or scripts.\n",
    "\n",
    "### Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b46537b-8eb5-4ce1-b01d-dbfcb5e00eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    model_kwargs = {\"response_format\" : { \"type\": \"json_object\" } }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68df024-89cd-4f0c-aa9c-e397993bade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"\"\"\n",
    "   Return a JSON object with two variables. The \"name\" is \"Joe Doe\" and his \"age\" is \"30\".\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe80fae-5e92-469c-bf60-40c4eea15ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f72af-4510-48fc-b99d-f6279ba73a1e",
   "metadata": {},
   "source": [
    "### Method 2:\n",
    "\n",
    "- JSON requires `{` and `}` for its syntax. To escape these in Python strings, use double braces {{ and }}.\n",
    "- `{name}` and `{age}` placeholders for variables remain single braces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03db585-97a6-4207-9b30-58734a7ea5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: enter appropriate curly braces below\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"name\", \"age\"],\n",
    "    template=\"\"\"You are an assistant that formats responses in JSON.\n",
    "Given the following inputs:\n",
    "- Name: {name}\n",
    "- Age: {age}\n",
    "\n",
    "Respond with a JSON object in the following format:\n",
    "_____\n",
    "    \"name\": \"value\",\n",
    "    \"age\": value\n",
    "_____\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1c103-6b0f-48cb-808b-c3e58a830328",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(name=\"John Doe\", age=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59e6a6-fcc0-4dd9-bc26-714dbf7c8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc8aea-ad2f-4202-a6bd-a59d01421eac",
   "metadata": {},
   "source": [
    "## Schema Definition\n",
    "\n",
    "- The output structure of a model response needs to be represented in some way.\n",
    "- The simplest and most common format is a JSON-like structure which you just seen.\n",
    "- The other method is use `Pydantic` as it allow you mto define schemas using Python's type annotations.\n",
    "- It validates that the LLM's output conforms to the expected structure, catching errors early.\n",
    "\n",
    "### Method 1: (Tool)\n",
    "Tool challing refers to the mechanism where the language model interacts with external tools or functions to enhance its capabilities. This approach allows the model to perform actions, retrieve specific information, or execute tasks that go beyond its built-in knowledge and reasoning.\n",
    "\n",
    "For a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a37201-8c76-474d-8b0a-ced95c50745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# TODO: ResponseFormatter inherits from BaseModel \n",
    "# BaseModel is the foundational class in the pydantic library\n",
    "class ResponseFormatter(BaseModel):\n",
    "    answer: str = Field(description = \"The answer to the user's question\")\n",
    "    followup_question: str = Field(description = \"A followup question the user could ask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb769fe7-a00d-477f-8bec-9f425940d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\", \n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# TODO: bind response formatter schema as a tool to the model\n",
    "model_with_tools = model.bind_tools([_____])\n",
    "\n",
    "# Invoke the model\n",
    "response = model_with_tools.invoke(\"What was the original colour of the Hulk in his first comic appearance?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f3594-0d48-42b3-92f0-8ac38a5bdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 'eval' method to extract followup_question\n",
    "\n",
    "followup = eval(response.additional_kwargs['tool_calls'][0]['function']['arguments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4cdf8-5e09-4012-b56e-9020bc81a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "followup['followup_question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05cd37-4584-44e9-a75a-f991e92bb337",
   "metadata": {},
   "source": [
    "### Method 2: (Pydantic)\n",
    "\n",
    "Specify an JSON schema and query LLM for JSON outputs that conform to that schema.\n",
    "\n",
    "Declare a data model with validation using Pydantic decoration `@validator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d0d72-58f5-4144-90fe-2f213b18fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.pydantic import PydanticOutputParser\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate\n",
    "\n",
    "# TODO: ResponseFormatter inherits from BaseModel. \n",
    "# BaseModel is the foundational class in the pydantic library\n",
    "class ResponseFormatter(BaseModel):\n",
    "    answer: str = Field(description = \"The answer to the user's question\")\n",
    "    followup_question: str = Field(description = \"A followup question the user could ask\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ResponseFormatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777365ce-95c3-43be-aba2-81b1d4128eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\", \n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fef609-ec20-45ec-9a18-6d4779ec28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "messages = prompt.format_prompt(\n",
    "    format_instructions=parser.get_format_instructions(),\n",
    "    query = \"What was the original colour of the Hulk in his first comic appearance?\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a5d02-0bc3-4178-9ade-02db9a6079e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386a0c7-6240-4538-b3a9-29658ed944c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the parse to extract the answer and followup question\n",
    "\n",
    "output = parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208eceb8-4ad5-46ed-abb0-5aea44ead1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3ff8c-44c1-4391-9aa5-baa6e1275f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.followup_question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217b258-cfb1-43d4-a6ed-73a21bd9bc25",
   "metadata": {},
   "source": [
    "### Method 3: (ResponseSchema)\n",
    "\n",
    "Schema for a response from a structured output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662bea5-85fa-4135-9cd4-d68e50bfc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.structured import ResponseSchema\n",
    "from langchain.output_parsers.structured import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880c481-a9c8-4e56-8b5c-90faff7a8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the schemas\n",
    "\n",
    "# TODO: create the response schemas\n",
    "answer_schema = _____(\n",
    "    name = \"answer\",\n",
    "    description = \"The answer to the user's question\"\n",
    ")\n",
    "\n",
    "followup_question_schema = _____(\n",
    "    name = \"followup_question\",\n",
    "    description = \"A follow up question the user could ask\"\n",
    ")\n",
    "\n",
    "# TODO: set up a list of schema just created\n",
    "response_schema = [\n",
    "    _____,\n",
    "    _____\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557233ab-82cb-4cac-b2d6-aa02a68238d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the output parser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# print the formatting instruction to understand the output format\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a2070-eddb-475a-adb6-04953a74c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "messages = prompt.format_prompt(\n",
    "    format_instructions=parser.get_format_instructions(),\n",
    "    query = \"What was the original colour of the Hulk in his first comic appearance?\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ba4cd-a37f-449a-a385-045b19bb07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the model\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d341f-eea1-48f7-b9a0-f13151c61d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)\n",
    "print('-'*10)\n",
    "\n",
    "# the content is of type 'str'\n",
    "print(type(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef00152-4610-45da-b0dc-93571d688044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# force the output to be type dict\n",
    "\n",
    "dict_response = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70490c-844e-4ef7-87e2-c8486f2a6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_response)\n",
    "print('-'*10)\n",
    "\n",
    "# the content is of type 'str'\n",
    "print(type(dict_response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9061644-c5e1-4278-848d-db0628bbd096",
   "metadata": {},
   "source": [
    "## Memory Persistence\n",
    "- LLMs are stateless.\n",
    "- Each incoming query is processed independently.\n",
    "- Memory allows a LLM to remember previous interactions.\n",
    "\n",
    "According to LangChain documentation, it is recommended to take advantage of LangGraph persistence to incorporate `memory` into new LangChain applications.\n",
    "\n",
    "- [Build a Chatbot](https://python.langchain.com/docs/tutorials/chatbot/#installation)\n",
    "- [How to add thread-level persistence to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)\n",
    "\n",
    "The `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationTokenBufferMemory` and `ConversationSummaryBufferMemory` methods are deprecated since version 0.3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835323a6-b99b-4933-ba8c-81fbed905660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba088b14-eac4-491c-a2a7-ff0fb4c7191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1519129c-3fee-45f7-9605-8a1acdd04de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1efa45-6499-47b4-a7c7-a56545b39f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# visualize the graph using the get_graph method and one of the \"draw\" methods, like draw_ascii or draw_png\n",
    "\n",
    "def drawGraph(graph):\n",
    "    try:\n",
    "        display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    except Exception:\n",
    "        # This requires some extra dependencies and is optional\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3073a6-146c-45e2-a8f3-a37e14c47715",
   "metadata": {},
   "source": [
    "### Without Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356f350-d43a-4df7-a31d-eae53f2ed80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# define the (single) node in the graph\n",
    "# TODO: fill in the blanks\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"_____\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adca85a-f329-4c04-b50c-4c286bcb6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the graph\n",
    "drawGraph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92546fca-7d82-4886-be73-84e08bd8d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm John.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22aa86-a40a-4dd1-9525-f24584e879a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e2f53-2d3c-4f4e-b7fa-856b6de9d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e4713-b23f-4dcc-8e65-f6f2e5787709",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68088a32-53b4-4c95-80fc-08f259a7e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a38e7-7269-4d43-ad18-c5cb7e585212",
   "metadata": {},
   "source": [
    "### With Persistence\n",
    "\n",
    "- To add in persistence, we need to pass in a `Checkpointer` when compiling the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045024c9-b71c-475c-ad7d-dfc5b126bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory\n",
    "# TODO: initialise memory saver\n",
    "memory = _____()\n",
    "# TODO: pass in the memory checkpointer\n",
    "app = workflow.compile(checkpointer=_____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebdc0af-80f6-4b3c-ba9b-759fae876865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the graph\n",
    "drawGraph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738fffd-41ef-442b-af80-8ee5a7482333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialise an arbitary thread ID of 1\n",
    "config = {\"configurable\": {\"thread_id\": \"_____\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c264ba-071b-4866-8d28-936c2205f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi! I'm John.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d493ae-b75b-476e-8645-e5155c086da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b7274-3e53-45fd-aa14-ff37104877e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16e27e-8653-46b1-b78a-264bba7b5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78112adc-afce-4670-8e08-f43e6f82655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d6f1f7-7e3b-4fef-9f8d-920ee61b6ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
