{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SST_DP2025/blob/main/L13/L13.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab (preferred).\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `SST_DP2025`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f4c2-8506-415c-a295-041ee40d9eda",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at Python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.tx')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab.\n",
    "\n",
    "## Tip\n",
    "The message output from LangChain/LangGraph can be difficult to read. Use [Python Formatter](https://codebeautify.org/python-formatter-beautifier) to format the output for easy reading."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c79ed1d-cc1e-4712-93ce-695496907dd7",
   "metadata": {},
   "source": [
    "# Lesson 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a344c6-9289-4351-9677-8e7f778c7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain\n",
    "%pip install --quiet -U langgraph\n",
    "%pip install --quiet -U langchain-openai\n",
    "%pip install --quiet -U langchain-community\n",
    "%pip isntall --quiet -U tavily-python\n",
    "%pip install --quiet -U wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f2767-9f87-48e8-b1b7-c0b952d2cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain              0.3.11\n",
    "# langgraph              0.2.59\n",
    "# langchain-core         0.3.24\n",
    "# langchain-openai       0.2.12\n",
    "# langchain-community    0.3.12\n",
    "# openai                 1.57.2\n",
    "# pydantic               2.10.3\n",
    "# tavily-python          0.5.0\n",
    "# wikipedia              1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78951ab-5382-49f1-a849-ad18ba311834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a9af7-9caf-46f0-8a11-2e627d8bb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the OpenAI API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c772b-582c-4585-8292-76f691dce7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goto https://tavily.com/ and sign up to get an API key\n",
    "\n",
    "# get Tavily Search API key ready and enter it when ask\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba823328-3b99-473c-aefb-b422982c595d",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "LangGraph has checkpoint that is a specific memory implementation which saves the state of the graph at various points of execution. The state in a graph is dynamically changed when executed. \n",
    "\n",
    "Persistent memory and checkpoints provides a way to store and retrieval historical states, allowing more complex workflows and human-in-the loop interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705457f9-d30d-4601-bc4a-f3f4926f40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23646280-e88c-42fd-bcfd-57092ccea5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the nodes function\n",
    "\n",
    "def multiply(a : int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add (a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"\n",
    "    Divide a and b\n",
    "    Args:\n",
    "       a: first int\n",
    "       b: second int\n",
    "    \"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d553b2-6fde-49eb-bad3-c6fe79998a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini'\n",
    ")\n",
    "\n",
    "# TODO: add the tools to the list\n",
    "arithmetic_tools = [_____, _____, _____]\n",
    "# TODO: binds the arithmetic tools\n",
    "model_with_tools = model.bind_tools(_____)\n",
    "\n",
    "# system message\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a helful assistant tasked with performing arithmetic operations.\"\n",
    ")\n",
    "\n",
    "# helpful assistant node\n",
    "def assistant(state: MessagesState):\n",
    "    return {\"messages\" : [model_with_tools.invoke([sys_msg] + state['messages'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc026a8-df0b-443b-9b85-160446e8b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "\n",
    "# TODO: build graph by filling in the blanks\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node('assistant', _____)\n",
    "graph_builder.add_node('tools', _____(_____))\n",
    "\n",
    "graph_builder.add_edge(_____, 'assistant')\n",
    "graph_builder.add_conditional_edges(\n",
    "    'assistant',\n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "graph_builder.add_edge('_____', '_____')\n",
    "without_memory_graph = graph_builder.compile()\n",
    "\n",
    "display(Image(without_memory_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ecd3f4-4b15-4fe8-abd2-f6d7a3a29f0c",
   "metadata": {},
   "source": [
    "### Without Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a094e0-6ba8-41f1-a607-35586c951f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_msg = [HumanMessage(content='Add 10 and 10.')]\n",
    "test_msg = without_memory_graph.invoke({'messages' : test_msg})\n",
    "\n",
    "for m in test_msg['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbe6e0-440a-4da3-ac82-e0f122dc5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's multiply by 2\n",
    "\n",
    "test_msg = [HumanMessage(content='Multiply by 10.')]\n",
    "test_msg = without_memory_graph.invoke({'messages' : test_msg})\n",
    "\n",
    "for m in test_msg['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742cdf07-7ca1-4648-9b8f-02ade777f626",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Observe that it cannot perform the multiuplication.  The output of `20` is not remembered.  This is because the state is transient to a single graph execution.\n",
    "\n",
    "To solve this issue, we can use `persistence`. Use a `checkpointer` to automatically save the graph state after each step.\n",
    "\n",
    "One of the checkpointers is `MemorySaver`. Simply compile the graph with a checkpointer and our graph has memory! When we use memory, we also need to specify a `thread_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637be77-2026-4ef7-bc1b-aba7e86ca59b",
   "metadata": {},
   "source": [
    "### With Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3eeda8-fb9a-4189-a340-23d6c7dc5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f7e93-009d-4ac2-9baf-77b14f456cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialise memory saver\n",
    "memory = _____()\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\" : str(random.randint(1, 100))}}\n",
    "\n",
    "# TODO: compile the graph with memory checkpointer\n",
    "with_memory_graph = graph_builder._____(checkpointer=______)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa1924-b3d4-4520-921e-6bcb44c18a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_msg = [HumanMessage(content='Add 10 and 10.')]\n",
    "test_msg = with_memory_graph.invoke({'messages' : test_msg}, thread)\n",
    "\n",
    "for m in test_msg['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36edfc-c029-46a1-acd1-5eac068ede64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's multiply by 2\n",
    "\n",
    "test_msg = [HumanMessage(content='Multiply by 10.')]\n",
    "test_msg = with_memory_graph.invoke({'messages' : test_msg}, thread)\n",
    "\n",
    "for m in test_msg['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba92665-6189-4adf-b56a-0daa41a23bba",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Observe the calculation is done from the initial one (addition) plus the latest step (multiply)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fed603-3163-4e04-9eae-0a534e555fe5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6aa9c157-f994-4ca5-a555-a1cd84e67671",
   "metadata": {},
   "source": [
    "## Parallelization\n",
    "\n",
    "Parallel node execution involves executing multiple nodes or tasks simultaneously rather than sequentially. This will be useful when tasks are independent of each other and can be processed in parallel to improve efficiency.\n",
    "\n",
    "For example, you are building a system to analyze a user query across multiple knowledge sources simultaneously, such as Wikipedia, news articles, and a proprietary database. Each knowledge source requires its own processing logic. Such tasks are suitable for parallel node execution.\n",
    "\n",
    "**Fan out** refers to a workflow pattern where a single input (or a small set of inputs) is distributed to multiple nodes or branches for parallel processing. Each node or branch works independently, often performing different tasks or processing the input in unique ways.\n",
    "\n",
    "**Fan in** is the reverse of fan-out. It refers to a pattern where multiple independent outputs or results from parallel branches are merged or combined into a single unified output. This is often used after a fan-out pattern to aggregate, reduce, or process the results of parallel computations into a cohesive response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5afdd-affa-4f88-aea0-7e9a6e975173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "import operator\n",
    "import random\n",
    "\n",
    "from typing import Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.errors import InvalidUpdateError\n",
    "\n",
    "from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd474d57-b192-4150-a7cb-00019aac1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    state: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fe783-aab7-4924-aa9d-a23b12800615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom node\n",
    "\n",
    "class ReturnNodeValue:\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['state']}\")\n",
    "        return {\"state\": [self._value]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f54a6-367c-490c-9278-327a8196d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes\n",
    "memory = MemorySaver()\n",
    "\n",
    "# TODO: build graph by filling in the blanks\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"_____\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"_____\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"_____\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"_____\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"_____\", \"_____\")\n",
    "builder.add_edge(\"_____\", \"_____\")\n",
    "builder.add_edge(\"_____\", \"_____\")\n",
    "builder.add_edge(\"_____\", \"_____\") \n",
    "builder.add_edge(\"d\", END)\n",
    "without_operator_add_graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(without_operator_add_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38b39f-62f6-4dba-9b97-b27af2172ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\" : str(random.randint(1, 100))}}\n",
    "\n",
    "try:\n",
    "    test_msg = without_operator_add_graph.invoke({'state' : []}, thread)\n",
    "except InvalidUpdateError as e:\n",
    "    print(f'\\nAn error has occurred: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227586d-e7fa-4658-9e54-e542e3deadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = []\n",
    "for state in without_operator_add_graph.get_state_history(thread):\n",
    "    print(state)\n",
    "    all_states.append(state)\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88833037-b785-4d99-b3b6-38ed57f06d67",
   "metadata": {},
   "source": [
    "### Observation\n",
    "The error is caused by contention. Nodes 'b' and 'c' are trying to update at the same time.\n",
    "\n",
    "When fanning out from node 'a', we need to ensure that we are using a `reducer` if the steps are writing to the same channel.  We need to apply `operator.add` to lists so that it performs a list concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7632c6ca-14a3-4590-a9b7-8ecedee26a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    state: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcc861-a723-41a1-97e7-0137fb80f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the same code as before\n",
    "\n",
    "# Add nodes\n",
    "memory = MemorySaver()\n",
    "\n",
    "# TODO: build graph by filling in the blanks\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Initialize each node with node_secret \n",
    "builder.add_node(\"_____\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_node(\"_____\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"_____\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"_____\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# Flow\n",
    "builder.add_edge(_____, \"a\")\n",
    "builder.add_edge(\"_____\", \"_____\")\n",
    "builder.add_edge(\"_____\", \"_____\")\n",
    "builder.add_edge(\"_____\", \"_____\")\n",
    "builder.add_edge(\"_____\", \"_____\") \n",
    "builder.add_edge(\"d\", _____)\n",
    "with_operator_add_graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(with_operator_add_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957ac3a-5a06-406c-bca8-f84d9e4b80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\" : str(random.randint(1, 100))}}\n",
    "\n",
    "try:\n",
    "    test_msg = with_operator_add_graph.invoke({'state' : []}, thread)\n",
    "except InvalidUpdateError as e:\n",
    "    print(f'\\nAn error has occurred: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb60332-b8b9-42ca-8762-e75fc023de5f",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Noticed above the strings are concatenated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae6796-7cec-4e0f-bfd3-385b9605b776",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2dc379-964f-43ed-92a3-b0faf1b80301",
   "metadata": {},
   "source": [
    "### Filtering and Trimming Messages\n",
    "\n",
    "**Filtering**\n",
    "Removes irrelevant or redundant information from messages. This helps the LLM to focus on the most critical parts of the interaction.\n",
    "\n",
    "**Trimming Messages**\n",
    "Many LLMs have token limits (e.g., 4,096 or 8,000 tokens for certain OpenAI models). Exceeding this limit can result in incomplete processing or errors. Shortens the length of messages to fit within constraints, like token limits for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87297a-94f3-42c9-a527-bab468b99675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, trim_messages\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57052ea3-d5f8-485d-9bb1-ea9b43246677",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [AIMessage(\"You said you want to learn quantum computing?\", name=\"Bot\", id=\"1\")]\n",
    "messages.append(HumanMessage(\"Yes. Please give me a quick summary to quantum computing.\", name=\"John\", id=\"2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b52bb-b196-44e4-83e5-45edc5c1bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890bb6fb-2ddc-416f-8abe-1e3e965ea1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a chat node\n",
    "def chat_node(state:MessagesState):\n",
    "    return {\"messages\" : model.invoke(state['messages'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491034d5-728c-40d9-96c5-c7e8115ee091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "# TOD: fill in the blanks\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(\"chat_node\", _____)\n",
    "graph_builder.add_edge(_____, \"chat_node\")\n",
    "graph_builder.add_edge(\"chat_node\", _____)\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# view graph\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b5b98-4fb7-401f-bea1-9dbbf3b5b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke graph\n",
    "\n",
    "msg = graph.invoke({'messages' : messages})\n",
    "\n",
    "for m in msg['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8347ee1f-d2c2-47ab-83ab-f7a40a0bbcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg\n",
    "\n",
    "# take a look at how many 'input tokens', 'output_tokens' and 'total_tokens' in the output\n",
    "# we will compare it after the trim_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253011e3-8654-48b1-bf30-1001e5967714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before trimming\n",
    "\n",
    "msg['messages'][2].usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54406702-602e-4e38-937f-8a5d3444f3ff",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "Don't pass in the last message in the queue (list) for the model (LLM) to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d22f68-d029-44e7-aa72-522881a6209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't want to process the last message. Call invoke without passing in the last message\n",
    "\n",
    "filtered_msg = graph.invoke({'messages' : messages[:-1]})\n",
    "\n",
    "for m in filtered_msg['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0323d5-c9c7-430a-a0fe-e7ee8fb399aa",
   "metadata": {},
   "source": [
    "### Trimming\n",
    "\n",
    "All models have finite context window. There is a limit on how many tokens the model can take as input. Long messages or a chain/ahent that accumulates a long history will cost more to process.\n",
    "\n",
    "`trim_messages` can be used to reduce the size of a chat history to a specified token count or specified message count.\n",
    "\n",
    "[API Reference: trim_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96d98a-6133-45c0-a660-e1326ed7adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "# trim_chat_node. Use trim_messages to trim\n",
    "def trim_chat_node(state: MessagesState):\n",
    "    trim_msg = trim_messages(\n",
    "        state['messages'],\n",
    "        max_tokens = 100,\n",
    "        #start_on = 'human',\n",
    "        #end_on = (\"human\", \"tool\"),\n",
    "        strategy = \"last\",\n",
    "\n",
    "        # use the same one as 'model' \n",
    "        token_counter = model,        \n",
    "        allow_partial = False,\n",
    "    )\n",
    "    \n",
    "    return {\"messages\": [model.invoke(trim_msg)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e1f97-00da-46b2-aa4e-5706cb6f0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "# TODO: fill in the blanks\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "graph_builder.add_node(\"trim_chat_node\", _____)\n",
    "graph_builder.add_edge(_____, \"trim_chat_node\")\n",
    "graph_builder.add_edge(\"trim_chat_node\", _____)\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# view graph\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2163618-1fe9-4349-9614-38c9b5edab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before trimming\n",
    "# observe the message history\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6697c8c-160a-4266-bdd8-b434fc45ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim the messages \n",
    "trimmed = graph.invoke({'messages': messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f116417-0b82-4b6e-8616-b9b657a0abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in trimmed['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4555c-5765-4ded-a93a-981dd72246ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed\n",
    "\n",
    "# take a look at how many 'input tokens', 'output_tokens' and 'total_tokens' in the output below\n",
    "# compare with the outputs from previous (before trimming)\n",
    "# you should see some reduction in the token count (after trimming) ?\n",
    "\n",
    "# is the trimmed output meet your expectation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54740e-6e09-488a-85c5-a67ef88513ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After trimming\n",
    "trimmed['messages'][2].usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5dff77-71a4-43af-8604-61a5f898b383",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73304e4-1790-44e1-b064-f13924a45340",
   "metadata": {},
   "source": [
    "## Reducer\n",
    "\n",
    "### add_messages\n",
    "`add_message(left: Messages, right: Messages, ...)`\n",
    "It merges two list of messages, updating existing message by ID.\n",
    "- The left side is the base list\n",
    "- The right side is the list of messages to merge into the base list\n",
    "\n",
    "[API Reference: add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#langgraph.graph.message.add_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec3b543-8284-4a0a-b266-23785a6bbc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, RemoveMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52cc28-7bd1-4e30-856f-1058bd16d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [AIMessage(content = \"Hello! How can I assist you?\", name=\"Bot\", id=\"1\")]\n",
    "messages.append(HumanMessage(\"I am looking for ultimate LangChain tutorial\", name=\"John\", id=\"2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a71e4-cf42-402a-b42b-227b74a38724",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56f131-fd58-4761-8b4e-03c7e6497a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bot_message = AIMessage(content = \"Any specific topics you desire?\", name=\"Bot\", id=\"3\")\n",
    "\n",
    "new_msg = add_messages(new_bot_message, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2e261-d3b0-4dfe-b1b8-850df1d96a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95cd3d4-e059-41e6-9d82-39575e68d351",
   "metadata": {},
   "source": [
    "### Re-writing\n",
    "\n",
    "There is a userful trick when working with `add_messages` reducer.  If you pass a message with the same ID as an existing one in our message list, it will get over written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81826e9d-5664-404c-b585-dd554788ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79306c2-d9be-411d-9a8a-b963c25910fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_message = AIMessage(content = \"What will you like to learn?\", name=\"Bot\", id=\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397efa7d-0c9c-4148-a803-8675cd3adaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_msg = add_messages(new_msg, replacement_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78f812-c1c7-41e2-a860-2832ee8d4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_msg\n",
    "\n",
    "# observe that the message \"Any specific topics you desire?\" has been replaced with \"What will you like to learn?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a983b-3f5a-4a78-a458-a25808fc3f10",
   "metadata": {},
   "source": [
    "### Removal\n",
    "\n",
    "To remove a message, we simply use `RemoveMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e13fa-40c1-47b4-8b69-5dc8aa227f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming we just want to remove the last message\n",
    "\n",
    "deleted_message = [RemoveMessage(id=m.id) for m in replaced_msg[:-2]]\n",
    "delete_msg = add_messages(replaced_msg, deleted_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ba9b1-fc84-45ed-babf-3314affa2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_msg\n",
    "\n",
    "# observe the last message \"What will you like to learn\" is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f81411-ee3d-4ac5-9e70-31a5bf62dbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
