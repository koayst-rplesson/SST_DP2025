{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SDGAI_LLMforGenAIApp_Labs/blob/main/L01/L01.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `c3669c`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79ed1d-cc1e-4712-93ce-695496907dd7",
   "metadata": {},
   "source": [
    "# Lesson 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a896f4-8509-456a-9a25-46532342f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa80c0ce-2883-4169-b775-9ab96cfaf4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    }
   ],
   "source": [
    "# print module version(s)\n",
    "\n",
    "import nltk\n",
    "print (nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118fabe-37b7-4cd4-b7a4-9b0fc3875ca3",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This step takes a piece of text and converts it into a list of tokens. If the input is a sentence, then separating the words (including punctuations) would be an example of tokenization. Depending on the model, different granularities can be chosen. At the lowest level, each character could be a token.\n",
    "\n",
    "Reference: https://www.nltk.org/api/nltk.tokenize.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ce76eb-980c-4ca8-a43f-3475b68a5e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\koay_seng_tian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# download and install the resource \n",
    "# https://www.nltk.org/api/nltk.tokenize.punkt.html\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecafe0d-164a-477d-96dc-2b9051360f85",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de7e77f-cc38-412d-8f8a-db788e00cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a205c6c-4b3d-43f5-ad45-00a66aa86629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Mr. John!', 'I am going to buy some vegetables from the supermarket.', 'Should I pick up some broccoli?']\n"
     ]
    }
   ],
   "source": [
    "text_tok = sent_tokenize(\"Hi Mr. John! I am going to buy some vegetables from the supermarket. Should I pick up some broccoli?\")\n",
    "\n",
    "print(text_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7bab35-48bf-4208-b6b5-be9326c2bcd6",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3810241-4afe-4e4b-881b-8df5bd5b5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589a4cdc-02e5-4f6d-a820-7d40dbf3becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "words_tok = word_tokenize(\"I am learning natural language processing.\")\n",
    "\n",
    "# print the tokens\n",
    "# notice punctuations like full stop is a token\n",
    "print(words_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d9f6a0-e394-4635-8d65-d6505d599814",
   "metadata": {},
   "source": [
    "## Remove Punctuation and Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c7411e-bbf3-4931-8cd8-929ed37b75f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generative', 'ai', 'refers', 'to', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'creates', 'new', 'content', 'such', 'as', 'text', 'images', 'music', 'or', 'even', 'code', 'by', 'learning', 'patterns', 'from', 'existing', 'data', 'it', 'uses', 'models', 'like', 'gpt', 'or', 'gans', 'to', 'generate', 'realistic', 'outputs', 'often', 'mimicking', 'human', 'creativity', 'this', 'technology', 'is', 'transforming', 'fields', 'like', 'content', 'creation', 'design', 'and', 'personalized', 'experiences', 'by', 'automating', 'tasks', 'that', 'traditionally', 'required', 'human', 'intervention']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"\"\"Generative AI refers to a branch of artificial intelligence that creates new content, \n",
    "such as text, images, music, or even code, by learning patterns from existing data. \n",
    "It uses models like GPT or GANs to generate realistic outputs, often mimicking human creativity. \n",
    "This technology is transforming fields like content creation, design, and personalized experiences \n",
    "by automating tasks that traditionally required human intervention.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "tokens_no_punctuation = []\n",
    "for tok in tokens:\n",
    "    if not(tok in string.punctuation):\n",
    "        tokens_no_punctuation.append(tok.lower())\n",
    "\n",
    "print(tokens_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c62102-2fc4-420a-98cf-cd06aed03f1d",
   "metadata": {},
   "source": [
    "## Stop Word\n",
    "\n",
    "Stop words are common words that are just used to support the sentence construction. Stop words are removed from our analysis as they do not impact the meaning of sentence they are present in. Examples of stop words include `a`, `am` and `the`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd5f12f-c3fd-4e13-bc0b-f93e79ea5c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\koay_seng_tian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download and install the resource \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591d3d22-f8d7-43db-a5d5-938617afbb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# to print the list of stopwords in English language\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53895d7a-ff5a-4948-a97f-1b569758c15c",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c1d977-21b6-4c6c-a99a-655a6b4f72cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am learning Python. It is one of the most popular programming language.\"\n",
    "sentence_words = word_tokenize(sentence)\n",
    "\n",
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15ff08ed-c9a4-4b8b-9c8a-7e99070e80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming language .\n"
     ]
    }
   ],
   "source": [
    "# the code below uses python list comprehension to construct the sentence_no_stop_word list\n",
    "sentence_no_stop_word = ' '.join([word for word in sentence_words if word not in stop_words])\n",
    "\n",
    "print(sentence_no_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e97c50-52cf-44fb-a761-83b40d23acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming language .\n"
     ]
    }
   ],
   "source": [
    "# the code below is the same as above code except it is not using 'python list comprehension' technique\n",
    "sentence_no_stop_word = []\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in stop_words:\n",
    "        pass\n",
    "    else:\n",
    "        sentence_no_stop_word.append(word)\n",
    "\n",
    "print(' '.join(sentence_no_stop_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234c7e5-0438-402d-8cb9-9b313886a734",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "In English, words get tranformed into various forms when being in a sentence. For example, the word `product` get tranformed into `production`. It is necessary to convert these words into their base forms as they carry the same meaning.\n",
    "\n",
    "|            | Stemming Process |         |\n",
    "|------------|:----------------:|---------|\n",
    "| Production |        =>        | product |\n",
    "| Products   |        =>        | product |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ba4833-6c50-4216-b130-4fef6249b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6fc5e2b-00e1-41c3-9a9e-618d09f38d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product\n",
      "product\n",
      "come\n",
      "fire\n",
      "battl\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"Production\"))\n",
    "print(stemmer.stem(\"Products\"))\n",
    "print(stemmer.stem(\"coming\"))\n",
    "print(stemmer.stem(\"firing\"))\n",
    "\n",
    "print(stemmer.stem(\"battling\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57cb08-395c-4862-91fe-88673c65766d",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "The stemming process, sometimes, leads to unusual result.  For example, the word 'battling' is transformed to 'battl\" which cannot be found in a dictionary. To overcome this issue, lemmatization is another technique to use. The base form of the word can be found in a dictionary. This additional check (is it a word in the dictionary?) slows down the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64933b80-43f4-4bcb-8d31-f9ef0c002814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\koay_seng_tian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f8fb2fc-ac63-42d5-b37f-3781a15fa7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30cced34-9c4d-45c0-ab8c-fcf1bd7e21bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production\n",
      "Products\n",
      "coming\n",
      "firing\n",
      "battling\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"Production\"))\n",
    "print(lemmatizer.lemmatize(\"Products\"))\n",
    "print(lemmatizer.lemmatize(\"coming\"))\n",
    "print(lemmatizer.lemmatize(\"firing\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"battling\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c4eff-d226-451d-82f6-6226d05c5962",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "445a40df-b8fe-4e58-a4bf-f5ae6afe7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# MWE stands for Multi-Word Expression.\n",
    "# Certain groups of multi words are treated as one entity during tokenization.\n",
    "# For example: \"United States of America\", \"not only\", \"but also\"\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "238ea16c-857a-445c-a529-a86d0e74aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_words = [(\"artificial\",\"intelligence\"),(\"data\",\"science\")]\n",
    "mwe_tokenizer = MWETokenizer(compound_words, separator='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45f4ff72-3215-41b2-8d5f-eb71ffe59d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = 'We need to invest in data science and artificial intelligence capabilities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37aa3666-96b0-4c1f-9206-2aced862e0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'need', 'to', 'invest', 'in', 'data_science', 'and', 'artificial_intelligence', 'capabilities']\n"
     ]
    }
   ],
   "source": [
    "tokens_words_mwe = mwe_tokenizer.tokenize(word_tokenize(text_1))\n",
    "\n",
    "print(tokens_words_mwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08f3f3f2-d557-4581-96e3-78a3d1fcb228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', 'Heng_Swee_Keat', 'will', 'deliver', 'a', 'Ministerial_Statement', 'on', 'additional', 'support', 'measures', 'for', 'COVID-19', 'pandemic', '.']\n"
     ]
    }
   ],
   "source": [
    "text_2 = \"\"\"Mr Heng Swee Keat will deliver a Ministerial Statement on additional   \n",
    "support measures for COVID-19 pandemic.\"\"\"\n",
    "\n",
    "compound_words = [(\"Ministerial\", \"Statement\"), (\"Heng\",\"Swee\",\"Keat\")]\n",
    "mwe_tokenizer = MWETokenizer(compound_words, separator='_')\n",
    "tokens_words_mwe = mwe_tokenizer.tokenize(word_tokenize(text_2))\n",
    "\n",
    "print(tokens_words_mwe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7927b0-9909-4e54-b997-ac49c1aeaa09",
   "metadata": {},
   "source": [
    "## POS (Part-of-Speech) Tagging\n",
    "\n",
    "POS refers to parts of speech. POS tagging refers to the process of tagging words within sentences into their respective parts of speech and then labelling them.\n",
    "\n",
    "|     | **NLTK** (not exhaustive)                                           |\n",
    "|-----|---------------------------------------------------------------------|\n",
    "| DT  | Determiner                                                          |\n",
    "| JJ  | Adjuctive                                                           |\n",
    "| NN  | Noun, common, singular or mass                                      |\n",
    "| PRP | Personal pronoun (e.g. he, she I, you)                              |\n",
    "| VBG | Verb, gerund or present participate (e.g. running, eating, walking) |\n",
    "| VBP | Verb, present tense (e.g. run, eat, walk)                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9ff80c2-194b-4ef6-ae57-d3abbee5dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\koay_seng_tian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# download and install the resource \n",
    "# https://www.nltk.org/_modules/nltk/tag/perceptron.html\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "581a6563-51f4-4d53-8edf-7abd9617cebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize the sentence and print the tokens\n",
    "words = word_tokenize(\"I am learning natural language processing.\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "046a1268-24d1-4b18-8f04-15720dea6810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('learning', 'VBG'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the POS tags\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7540cb4-3f2b-4968-be58-cb9fb62b885a",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Given the text below, what are the preprocessing techniques you could apply?\n",
    "\n",
    "\"Generative AI is revolutionizing many industries by automating creative tasks such as writing, designing, and even composing music. These models, like GPT and DALL·E, learn from vast amounts of data to produce original content that mimics human output. However, the ethical implications of this technology, including concerns about bias and intellectual property, are raising important debates. As generative AI continues to evolve, it is crucial for researchers and developers to address these challenges to ensure responsible and fair use.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddebc6-4dd4-47ca-807e-744a8a6cbab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
